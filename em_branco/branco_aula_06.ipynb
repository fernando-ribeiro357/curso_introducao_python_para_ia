{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":7030,"status":"ok","timestamp":1726604875681,"user":{"displayName":"Adriano Vasconcellos Soares","userId":"14599394622412668736"},"user_tz":180},"id":"msmti2Gmji7t","outputId":"c40f502a-16aa-4366-9ac0-5f724cea0314"},"outputs":[],"source":["!pip install langchain==0.3.0\n","!pip install langchain-groq==0.2.0"]},{"cell_type":"markdown","metadata":{"id":"UfBsPqTlOtZl"},"source":["# Aula 6: Criando nosso primeiro ChatBot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuUtqgmn4ZV0"},"outputs":[],"source":["import os\n","from langchain_groq import ChatGroq\n","from langchain.prompts import ChatPromptTemplate\n","\n","api_key = 'ADICIONE SUA API KEY DA GROQ'\n","os.environ['GROQ_API_KEY'] = api_key\n","\n","chat = ChatGroq(model='llama-3.1-70b-versatile')\n","\n","\n","def resposta_do_bot(lista_mensagens):\n","  template = ChatPromptTemplate.from_messages(\n","      [('system', 'Você é um assistente amigável chamado Asimo')] +\n","      lista_mensagens\n","  )\n","  chain = template | chat\n","  return chain.invoke({}).content\n","\n","print('Bem-vindo ao ChatBot da Asimo! (Digite x se você quiser sair!)\\n')\n","mensagens = []\n","while True:\n","  pergunta = input('Usuário: ')\n","  if pergunta.lower() == 'x':\n","    break\n","  mensagens.append(('user', pergunta))\n","  resposta = resposta_do_bot(mensagens)\n","  mensagens.append(('assistant', resposta))\n","  print(f'Bot: {resposta}')\n","\n","print('\\nMuito obrigado por utilizar o AsimoBot!')\n"]},{"cell_type":"markdown","metadata":{"id":"QILqGqsUY0cI"},"source":["## Trabalhando com templates de Prompts"]},{"cell_type":"markdown","metadata":{"id":"17gmkZ1Me0Ye"},"source":["### O que é um prompt?"]},{"cell_type":"markdown","metadata":{"id":"wPrmdICIebLE"},"source":["Um prompt é uma instrução ou uma solicitação que você fornece a um modelo de linguagem para gerar uma resposta ou um texto. Ele serve como um ponto de partida para a interação, orientando o modelo sobre o que você deseja que ele produza. Resumindo, o prompt é a entrada na forma de texto que é processada pelo modelo para geração de uma resposta. Ele deve sempre ser claro e específico.\n","\n","Para entender melhor, imagine que você está conversando com um chef de cozinha. Se você disser \"Faça algo gostoso\", o chef pode interpretar isso de várias maneiras, resultando em pratos diferentes. No entanto, se você especificar \"Prepare uma lasanha vegetariana\", o chef terá uma direção clara e poderá criar exatamente o que você deseja. Da mesma forma, um prompt bem formulado fornece ao modelo de linguagem uma orientação precisa, permitindo que ele gere respostas mais relevantes e alinhadas com suas expectativas."]},{"cell_type":"markdown","metadata":{"id":"bHOhF0xmfDzU"},"source":["### O que é um template de Prompt?"]},{"cell_type":"markdown","metadata":{"id":"RRvBrcDLY3lX"},"source":["Um template de prompt (ou PromptTemplate) é uma estrutura pré-definida que permite criar prompts de forma mais organizada e reutilizável ao interagir com modelos de linguagem. Ele funciona como um molde que pode ser preenchido com variáveis ou informações específicas, facilitando a geração de prompts consistentes e adaptáveis para diferentes contextos ou tarefas. Ao utilizar um PromptTemplate, você pode economizar tempo e garantir que a formulação do prompt siga um padrão desejado, o que pode melhorar a qualidade das respostas geradas pelo modelo. Essa abordagem é especialmente útil em aplicações onde você precisa gerar múltiplos prompts semelhantes, como em chatbots como é o nosso caso.\n","\n","Vamos a exemplos..."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"x_ijnY4CeGDw"},"outputs":[{"name":"stdout","output_type":"stream","text":["input_variables=['expressao', 'lingua'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['expressao', 'lingua'], input_types={}, partial_variables={}, template='Traduza \"{expressao}\" para {lingua}'), additional_kwargs={})]\n"]},{"data":{"text/plain":["ChatPromptValue(messages=[HumanMessage(content='Traduza \"Beleza?\" para inglês', additional_kwargs={}, response_metadata={})])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from langchain.prompts import ChatPromptTemplate\n","\n","template = ChatPromptTemplate.from_messages(\n","    [('user', 'Traduza \"{expressao}\" para {lingua}')]\n",")\n","print(template)\n","template.invoke({'expressao': 'Beleza?', 'lingua': 'inglês'})\n"]},{"cell_type":"markdown","metadata":{"id":"FvDP0KhljFHN"},"source":["Uma \"chain\" (ou cadeia) refere-se a uma sequência de operações ou etapas que são encadeadas para processar informações e gerar resultados de forma estruturada. Cada etapa na chain pode envolver diferentes componentes, como prompts, modelos de linguagem, ferramentas de processamento de dados ou chamadas a APIs, permitindo que construamos fluxos de trabalho complexos e personalizados."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"WOkp-C2qeIrx"},"outputs":[{"name":"stdout","output_type":"stream","text":["A tradução de \"Beleza?\" para francês é \"D'accord?\" ou apenas \"D'accord\". No entanto, se você quiser uma tradução mais literal, seria \"Beauté?\" ou \"C'est beau?\".\n","\n","Mas, se você está se referindo a um acordo ou concordância, como em \"Ok, beleza\", a tradução mais comum seria \"D'accord\".\n"]}],"source":["import os\n","from langchain_groq import ChatGroq\n","from langchain.prompts import ChatPromptTemplate\n","\n","api_key = 'sua-api-key-aqui'\n","os.environ['GROQ_API_KEY'] = api_key\n","\n","chat = ChatGroq(model='llama-3.1-70b-versatile')\n","\n","template = ChatPromptTemplate.from_messages(\n","    [('user', 'Traduza \"{expressao}\" para a língua {lingua}')]\n",")\n","\n","chain = template | chat\n","\n","resposta = chain.invoke({'expressao': 'Beleza?', 'lingua': 'franceza'})\n","\n","print(resposta.content)"]},{"cell_type":"markdown","metadata":{"id":"UhGfgFH-pfL1"},"source":["## Continuando o projeto lá da aula 3"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3RZ-kFklw2oL"},"outputs":[{"name":"stdout","output_type":"stream","text":["Bem vindo ao ChatBot. 🤖 Como posso te ajudar?\n","\n","Saindo do ChatBot...\n","\n","Obrigado por me consultar, estarei sempre aqui para te ajudar. 🤖\n","\n"]}],"source":["import os\n","from langchain_groq import ChatGroq\n","from langchain.prompts import ChatPromptTemplate\n","\n","api_key = 'SUA CHAVE DA GROQ AQUI'\n","os.environ['GROQ_API_KEY'] = api_key\n","chat = ChatGroq(model='llama-3.1-70b-versatile')\n","\n","def resposta_bot(mensagens):\n","    mensagens_modelo = [('system', 'Você é um assistente amigável chamado Rodney Lataria')]\n","    mensagens_modelo += mensagens\n","    template = ChatPromptTemplate.from_messages(mensagens_modelo)\n","    chain = template | chat    \n","    return chain.invoke({}).content\n","\n","print('Bem vindo ao ChatBot. 🤖 Como posso te ajudar?')\n","mensagens = []\n","while True:\n","    pergunta = input(f'[x: sair]\\n🙂 >>> ')\n","\n","    if pergunta.lower() == 'x':\n","        print('\\nSaindo do ChatBot...\\n')\n","        break\n","\n","    mensagens.append(('user', pergunta))\n","    resposta = resposta_bot(mensagens)\n","    mensagens.append(('assistant', resposta))\n","    print('''\n","    ───████████───\n","    ──▐▀▀▀▀▀▀▀▀▌──\n","    ─▐▐  ▀  ▀  ▌▌─\n","    ──▐──▄▄▄▄──▌──\n","    ──▐  █▄▄█  ▌──\n","    ''')\n","    print(f'>>> {resposta}\\n')\n","\n","print('Obrigado por me consultar, estarei sempre aqui para te ajudar. 🤖\\n')\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNdUJ4Pg1LRFu7Q4YmuZznX","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
